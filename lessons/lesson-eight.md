# ğŸ“˜ Lesson 8 â€“ Clustering (Unsupervised Learning)

âœ… In **Lesson 4**, we studied Regression (predicting numbers).
âœ… In **Lesson 6**, we studied Classification (predicting categories).
âœ… In **Lesson 7**, we practiced Spam Detection with Logistic Regression and Random Forest.

ğŸ‘‰ **Now in Lesson 8, we explore Clustering â€“ the first major Unsupervised Learning method.**

---

## ğŸ¯ Learning Objectives

By the end of this lesson, students will be able to:

* Understand what **Clustering** is and how it differs from supervised learning.
* Learn about common **Clustering algorithms**:

  * K-Means
  * Hierarchical Clustering
  * DBSCAN
* Understand how to **evaluate clustering models** using metrics:

  * Elbow Method
  * Silhouette Score
  * Daviesâ€“Bouldin Index

---

## 1) Introduction

In supervised learning, we train models with **labeled data** (we know the answers).

But what if we donâ€™t have labels?
ğŸ‘‰ Thatâ€™s where **Unsupervised Learning** comes in.

* **Clustering** = grouping data points into clusters based on similarity.
* The algorithm has no labels; it must **discover hidden patterns**.

---

## 2) What is Clustering?

**Definition:**

* Clustering is a method of unsupervised learning where the algorithm automatically groups data into clusters such that **items in the same cluster are more similar to each other than to those in other clusters**.

---

### Real-World Examples

* ğŸ›’ **Market Segmentation** â€“ group customers by purchasing behavior.
* ğŸ“± **App Users** â€“ group users into active vs casual usage clusters.
* ğŸ§¬ **Genetics** â€“ group DNA sequences based on similarity.
* ğŸ“· **Image Segmentation** â€“ group pixels into regions (sky, land, water).

---

### Contrast with Supervised Learning

* **Supervised (Regression/Classification):**

  * Needs **labels** (prices, categories).
  * Example: Spam/Not Spam.

* **Unsupervised (Clustering):**

  * No labels, algorithm finds structure.
  * Example: Grouping customers into segments.

ğŸ“Œ **Key Point:**
Supervised = *with answers*.
Unsupervised = *find the hidden patterns yourself*.

---

## 3) Clustering Algorithms

### a) K-Means Clustering

* **Idea:** Partition data into *k* clusters.
* **Steps:**

  1. Choose number of clusters (*k*).
  2. Randomly assign cluster centers (centroids).
  3. Assign each point to nearest centroid.
  4. Recalculate centroids.
  5. Repeat until stable.
* **Best for:** Spherical/round clusters, large datasets.
* **Limitation:** Must pick *k* beforehand, struggles with irregular shapes & noise.

---

### b) Hierarchical Clustering

* **Idea:** Build a **tree of clusters** (dendrogram).
* **Types:**

  * **Agglomerative (bottom-up):** Start with each point â†’ merge step by step.
  * **Divisive (top-down):** Start with all points â†’ split step by step.
* **Best for:** Understanding data relationships at multiple levels.
* **Limitation:** Computationally expensive, sensitive to noise.

---

### c) DBSCAN (Density-Based Spatial Clustering of Applications with Noise)

* **Idea:** Group dense regions, mark sparse points as noise.
* **Steps:**

  1. Pick Îµ (neighborhood size) + minPts (minimum points in a dense region).
  2. Core points form clusters if density â‰¥ minPts.
  3. Points not belonging to any cluster â†’ **noise/outliers**.
* **Best for:** Irregular shapes, outlier detection.
* **Limitation:** Sensitive to parameter choice, struggles if density varies a lot.

---

ğŸ“Œ **Important Note:**
Clustering algorithms are **not limited** to these three.
ğŸ‘‰ Others include **Gaussian Mixture Models (GMM), Mean-Shift, Spectral Clustering**.

The key is to **understand how clustering works** and choose the algorithm that fits your data.

---

## 4) Clustering Metrics

Unlike supervised ML (where you check accuracy, precision, etc.), clustering has no â€œground truthâ€ labels.
ğŸ‘‰ So we use **special metrics** to measure how well clusters are formed.

---

### 1) Elbow Method (for K-Means)

**Goal:** Find the best number of clusters (`k`).

**Steps:**

1. Run K-Means with different values of `k` (e.g., 1 â†’ 10).
2. Calculate **SSE (Sum of Squared Errors)** = how far points are from their cluster centers.
3. Plot `k` vs. SSE.
4. Look for the **â€œelbow pointâ€** â€” where SSE stops dropping sharply.

ğŸ‘‰ Example:

* k=2 â†’ SSE=400
* k=3 â†’ SSE=250
* k=4 â†’ SSE=180
* k=5 â†’ SSE=175

At **k=4**, improvement slows down â†’ 4 clusters is best.

ğŸ“Œ **Use when:** You want to estimate the right number of clusters for K-Means.

---

### 2) Silhouette Score

**Goal:** Measure how good the cluster separation is.

**Formula (intuitive idea):**

* Score = (Separation from other clusters â€“ Cohesion inside cluster) / max
* Range: **-1 to +1**

**Interpretation:**

* **+1** â†’ Perfect clustering (well-separated).
* **0** â†’ Overlapping clusters.
* **-1** â†’ Wrong clustering (points in wrong groups).

ğŸ‘‰ Example:

* Silhouette = 0.65 â†’ Good separation.
* Silhouette = 0.25 â†’ Weak separation.
* Silhouette = -0.1 â†’ Bad clustering.

ğŸ“Œ **Use when:** You want a single number to evaluate cluster quality.

---

### 3) Daviesâ€“Bouldin Index

* Measures **average similarity between clusters**.
* Lower score = better separation.

---

### Quick Comparison

| Metric           | What it Measures                | Good Value Means | Typical Use                |
| ---------------- | ------------------------------- | ---------------- | -------------------------- |
| **Elbow Method** | SSE drop vs `k`                 | Clear â€œbendâ€     | Finding number of clusters |
| **Silhouette**   | Cohesion + Separation           | Close to +1      | Overall cluster quality    |
| **DB Index**     | Avg similarity between clusters | Lower is better  | Compare clustering models  |

---

ğŸ‘‰ **Key Idea:**

* **Elbow Method** = choose best `k`.
* **Silhouette Score** = check cluster quality.
* **DB Index & others** = compare multiple clustering solutions.

---

## âœ… Lesson Summary

* **Clustering** = unsupervised learning for grouping data.
* Main algorithms: **K-Means, Hierarchical, DBSCAN**.
* Evaluation metrics: **Elbow Method, Silhouette Score, Daviesâ€“Bouldin Index**.
* Key difference:

  * Supervised = with labels.
  * Unsupervised = without labels.

---

## ğŸ”š End of Lesson 8

ğŸ‰ Congratulations! Youâ€™ve completed **Clustering â€“ your first Unsupervised Learning method in Machine Learning.**

---